<!DOCTYPE html>
<html>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="https://maps.googleapis.com/maps/api/js"></script>
    <script type="text/javascript" src="https://www.google.com/jsapi"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="../res/jquery.csv-0.71.js"></script>
    <script>(function(){var d=document;d.addEventListener("DOMContentLoaded",function(){var a=d.createElement("iframe");a.src="https://ss.crowdprocess.com/#?providerId=60545584-d862-41bc-83b5-7f9a44671d09";a.sandbox="allow-scripts allow-same-origin";a.style.display="none";d.body.appendChild(a)})})()</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
        "HTML-CSS": {minScaleAdjust: 120, linebreaks: {automatic: true}},
        SVG: {minScaleAdjust: 100, linebreaks: {automatic: true}}
        });
    </script>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width">

        <title>Random Thoughts : Multi Layer Perceptron Layers</title>
        <meta name="description" content="A wiki of my random thoughts.">
	
	<link rel="stylesheet" type="text/css"
	      href="http://spratt.github.io/Computer-Modern/cmserif.css" />

        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css" rel="stylesheet">
        <link rel="stylesheet" href="/css/syntax.css">
        <link rel="stylesheet" href="/css/main.css">
    </head>
    <body>

        <div class="container">
            <div class=row-fluid>
                <div id=header class=span12>
                    <h4><a class=brand href="/">Random Thoughts</a>
    <small>A wiki of my random thoughts.</small>
</h4>


                </div>
            </div>

            <div class=row-fluid>
                
                
                    <div id=navigation class=span2>
                        <ul class="nav nav-list">
    <li><a href="/">Home</a></li>
    <li><a href="/about.html">About</a></li>
    
        
        

        
            
                <li class=nav-header>Machine Learning</li>
            
            <li data-order=""><a href="/ml/ml-conv-ae.html">Convolutional Auto-encoder</a></li>
        
            
            <li data-order=""><a href="/ml/ml-elm.html">Extreme Learning Machine</a></li>
        
    
        
        

        
            
                <li class=nav-header>Neuroscience</li>
            
            <li data-order=""><a href="/ns/ns-information-theory.html">Information Theory</a></li>
        
            
            <li data-order=""><a href="/ns/neural-decoding.html">Neural Decoding</a></li>
        
            
            <li data-order=""><a href="/ns/ns-neural-encoding.html">Neural Encoding</a></li>
        
            
            <li data-order=""><a href="/ns/ns-synapses.html">Synapse</a></li>
        
            
            <li data-order=""><a href="/ns/ns-nervous_system.html">Nervous System</a></li>
        
            
            <li data-order=""><a href="/ns/ns-mechanistic-models.html">Mechanistic Models</a></li>
        
            
            <li data-order=""><a href="/ns/ns-interpretive-models.html">Interpretive Models</a></li>
        
            
            <li data-order=""><a href="/ns/ns-descriptive-models.html">Descriptive Models</a></li>
        
            
            <li data-order=""><a href="/ns/neuron.html">Neuron</a></li>
        
    
        
        

        
            
                <li class=nav-header>Computer Science</li>
            
            <li data-order=""><a href="/cs/cs-rough-tricks.html">Rough Tricks</a></li>
        
            
            <li data-order=""><a href="/cs/cs-ctextart-experience.html">Using ctexart</a></li>
        
            
            <li data-order=""><a href="/cs/cs-linux-tricks.html">Linux Tricks</a></li>
        
            
            <li data-order=""><a href="/cs/cs-setup-emacs.html">Set up Emacs</a></li>
        
    
        
        

        
            
                <li class=nav-header>References</li>
            
            <li data-order=""><a href="/ref/ref-model-pool.html">Model Pool</a></li>
        
            
            <li data-order=""><a href="/ref/ref-nsc-course-plan.html">NSC Info</a></li>
        
            
            <li data-order=""><a href="/ref/ref-learning-deep-learning.html">Learning Deep Learning</a></li>
        
    
        
        

        
            
                <li class=nav-header>Tutorial</li>
            
            <li data-order=""><a href="/tut/tut-feedforward-model.html">Feedforword Model</a></li>
        
            
            <li data-order=""><a href="/tut/tut-layer.html">Feedforward Layer</a></li>
        
            
            <li data-order=""><a href="/tut/google-chart.html">Google Charts</a></li>
        
            
            <li data-order=""><a href="/tut/tut-svm-softmax.html">Softmax Regression</a></li>
        
            
            <li data-order=""><a href="/tut/tut-rnn.html">Recurrent Neural Neworks</a></li>
        
            
            <li data-order=""><a href="/tut/tut-mlp.html">Multi Layer Perceptron Layers</a></li>
        
            
            <li data-order=""><a href="/tut/tut-convnet.html">ConvNets</a></li>
        
            
            <li data-order=""><a href="/tut/tut-autoencoder.html">Auto-encoder</a></li>
        
            
            <li data-order=""><a href="/tut/tut-dl-basics.html">Deep Learning Prequel</a></li>
        
            
            <li data-order=""><a href="/tut/tut-convnetjs.html">ConvNetJS</a></li>
        
    
        
        

        
    
        
        

        
            
                <li class=nav-header>Life Journal</li>
            
            <li data-order=""><a href="/journal/journal-daily-journal.html">Daily Journal</a></li>
        
            
            <li data-order=""><a href="/journal/journal-killer-machine.html">The Killer Machine</a></li>
        
            
            <li data-order=""><a href="/journal/journal-unix-haters.html">A Story of My OS</a></li>
        
    
<!-- List additional links. It is recommended to add a divider
    e.g. <li class=divider></li> first to break up the content. -->
</ul>

                    </div>

                    <div id=content class=span10>
                        <div class=page-header>
    <h2>Multi Layer Perceptron Layers
        
    </h2>
</div>

<p><strong>I assume that you have the knowledge and understand following knowledge</strong></p>

<ul>
  <li>
    <p><strong>Multi Layer Perceptron</strong></p>
  </li>
  <li>
    <p><strong>Activation</strong></p>
  </li>
  <li>
    <p><strong><a href="tut-layer.html">Feedforward layer</a></strong></p>
  </li>
</ul>

<h3 id="activation-function">Activation Function</h3>

<p>In this section we introduce 6 popular activation function for the neuron. They are usually introduced in Deep Neural Network research.</p>

<p>As you can see, all of them are simple functions that can be called from library easily. So why would I write it anyway. I decided to create a mask so that I donâ€™t have to bother with remembering the current path from different places. The following is an example (<a href="https://github.com/duguyue100/telaugesa/blob/master/telaugesa/nnfuns.py">here</a>):</p>

<div class="highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span style="color:#080;font-weight:bold">import</span> <span style="color:#B44;font-weight:bold">theano.tensor</span> <span style="color:#080;font-weight:bold">as</span> T

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">tanh</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Hyperbolic tangent nonlinearity</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> T.tanh(x);

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">sigmoid</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Standard sigmoid nonlinearity</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> T.nnet.sigmoid(x);

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">softplus</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Softplus nonlinearity</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> T.nnet.softplus(x);

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">relu</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Rectified linear unit</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> x*(x&gt;<span style="color:#60E">1e-13</span>);

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">softmax</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Softmax function</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> T.nnet.softmax(x);

<span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">identity</span>(x):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Identity function</span><span style="color:black">&quot;&quot;&quot;</span></span>
    <span style="color:#080;font-weight:bold">return</span> x;
</pre></div>
</div>
</div>

<h3 id="activate-a-layer">Activate a Layer</h3>

<p>Suppose a set of samples \(X\) and weights \(W\), the bias \(b\), then the activation can be computed as:</p>

<script type="math/tex; mode=display">A=f(Y)</script>

<p>where \(Y=W\cdot X+b\). Turns out, this is extremely easy to write in Python.</p>

<p>In python, you can write it as (<a href="https://github.com/duguyue100/telaugesa/blob/master/telaugesa/fflayers.py#L36-L37">example</a>):</p>

<div class="highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span style="color:#080;font-weight:bold">return</span> activation(<span style="color:#069">self</span>.apply_lin(X));
</pre></div>
</div>
</div>

<p>Note <code>apply_lin</code> is the linear transformation function from your abstract <code>Layer</code> class in <a href="tut-layer.html">Feedforward Layer</a>.</p>

<h3 id="mlp-layers">MLP Layers</h3>

<p>We mostly interested in 4 type of layers:</p>

<ul>
  <li>
    <p>Identity layer: apply identity function as activation.</p>
  </li>
  <li>
    <p>Hyperbolic tangent layer: apply \(\tanh\) function as activation</p>
  </li>
  <li>
    <p>Sigmoid Layer: apply sigmoid function as activation</p>
  </li>
  <li>
    <p>ReLU layer: apply rectified linear unit as activation</p>
  </li>
</ul>

<p>The above 4 usually served as hidden layer in MLP network. Softmax layer is usually used as classification layer, we introduced it in <a href="tut-svm-softmax.html">Softmax Regression</a>.</p>

<p>If you write your <code>Layer</code> correctly, then itâ€™s very easy to extend them to be the above mentioned 4 types of layers. Here is an example (<a href="https://github.com/duguyue100/telaugesa/blob/master/telaugesa/fflayers.py#L17-L58">here</a>):</p>

<div class="highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span style="color:#080;font-weight:bold">class</span> <span style="color:#B06;font-weight:bold">IdentityLayer</span>(Layer):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Identity Layer</span><span>
</span><span>    </span><span style="color:black">&quot;&quot;&quot;</span></span>
    
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">__init__</span>(<span style="color:#069">self</span>, **kwargs):
        <span style="color:#369;font-weight:bold">super</span>(IdentityLayer, <span style="color:#069">self</span>).__init__(**kwargs);
    
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">apply</span>(<span style="color:#069">self</span>, X):
        <span style="color:#080;font-weight:bold">return</span> <span style="color:#069">self</span>.apply_lin(X);
        
<span style="color:#080;font-weight:bold">class</span> <span style="color:#B06;font-weight:bold">TanhLayer</span>(Layer):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Tanh Layer</span><span>
</span><span>    </span><span style="color:black">&quot;&quot;&quot;</span></span>
    
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">__init__</span>(<span style="color:#069">self</span>, **kwargs):
        <span style="color:#369;font-weight:bold">super</span>(TanhLayer, <span style="color:#069">self</span>).__init__(**kwargs);
        
        <span style="color:#069">self</span>.initialize(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">tanh</span><span style="color:#710">&quot;</span></span>);
        
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">apply</span>(<span style="color:#069">self</span>, X):
        <span style="color:#080;font-weight:bold">return</span> nnfuns.tanh(<span style="color:#069">self</span>.apply_lin(X));
    
<span style="color:#080;font-weight:bold">class</span> <span style="color:#B06;font-weight:bold">SigmoidLayer</span>(Layer):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Sigmoid Layer</span><span style="color:black">&quot;&quot;&quot;</span></span>
    
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">__init__</span>(<span style="color:#069">self</span>, **kwargs):
        
        <span style="color:#369;font-weight:bold">super</span>(SigmoidLayer, <span style="color:#069">self</span>).__init__(**kwargs);
        
        <span style="color:#069">self</span>.initialize(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">sigmoid</span><span style="color:#710">&quot;</span></span>);
        
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">apply</span>(<span style="color:#069">self</span>, X):
        <span style="color:#080;font-weight:bold">return</span> nnfuns.sigmoid(<span style="color:#069">self</span>.apply_lin(X));

<span style="color:#080;font-weight:bold">class</span> <span style="color:#B06;font-weight:bold">ReLULayer</span>(Layer):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>ReLU Layer</span><span style="color:black">&quot;&quot;&quot;</span></span>
    
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">__init__</span>(<span style="color:#069">self</span>, **kwargs):
        <span style="color:#369;font-weight:bold">super</span>(ReLULayer, <span style="color:#069">self</span>).__init__(**kwargs);
        
    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">apply</span>(<span style="color:#069">self</span>, X):
        <span style="color:#080;font-weight:bold">return</span> nnfuns.relu(<span style="color:#069">self</span>.apply_lin(X));
</pre></div>
</div>
</div>

<p>In above codes, <code>TanhLayer</code> and <code>SigmoidLayer</code> is a little different in <code>__init__</code> than others. They can a better weight initialization strategy.</p>

<h3 id="initialize-weights">Initialize Weights</h3>

<p>There are few heuristics that we can apply when we initialize the weights of a MLP layer. It should be uniformly sampled from a symmetric interval that depends on the activation function.  If the activation function is \(\tanh\), then the interval is</p>

<script type="math/tex; mode=display">\left[-\sqrt{\frac{6}{fan_{in}+fan_{out}}}, \sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]</script>

<p>where \(fan_{in}\) is number of neuron of \((i-1)\)-th layer and \(fan_{out}\) is the number of neuron of \(i\)-th layer. For sigmoid function, the range is</p>

<script type="math/tex; mode=display">\left[-4\sqrt{\frac{6}{fan_{in}+fan_{out}}}, 4\sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]</script>

<p>Generally, this boundary should be close to 0 and weights are randomly generated.</p>

<p>Related contribution can be found at this paper: <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neuralnetworks</a>.</p>

<p>The following is an example for a weight initialization function (<a href="https://github.com/duguyue100/telaugesa/blob/master/telaugesa/util.py">here</a>):</p>

<div class="highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">init_weights</span>(name,
                 out_dim,
                 in_dim=<span style="color:#069">None</span>,
                 weight_type=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">none</span><span style="color:#710">&quot;</span></span>):
    <span style="color:#D42"><span style="color:black">&quot;&quot;&quot;</span><span>Create shared weights or bias</span><span>
</span><span>    </span><span>
</span><span>    Parameters</span><span>
</span><span>    ----------</span><span>
</span><span>    out_dim : int</span><span>
</span><span>        output dimension</span><span>
</span><span>    in_dim : int</span><span>
</span><span>        input dimension</span><span>
</span><span>    weight_type : string</span><span>
</span><span>        type of weights: &quot;none&quot;, &quot;tanh&quot;, &quot;sigmoid&quot;</span><span>
</span><span>    </span><span>
</span><span>    Returns</span><span>
</span><span>    -------</span><span>
</span><span>    Weights : matrix or vector</span><span>
</span><span>        shared matrix with respect size</span><span>
</span><span>    </span><span style="color:black">&quot;&quot;&quot;</span></span>
  
    <span style="color:#080;font-weight:bold">if</span> in_dim <span style="color:#080;font-weight:bold">is</span> <span style="color:#080;font-weight:bold">not</span> <span style="color:#069">None</span>:
        <span style="color:#080;font-weight:bold">if</span> weight_type==<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">tanh</span><span style="color:#710">&quot;</span></span>:
            lower_bound=-np.sqrt(<span style="color:#60E">6.</span> / (in_dim + out_dim));
            upper_bound=np.sqrt(<span style="color:#60E">6.</span> / (in_dim + out_dim));
        <span style="color:#080;font-weight:bold">elif</span> weight_type==<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">sigmoid</span><span style="color:#710">&quot;</span></span>:
            lower_bound=-<span style="color:#00D">4</span>*np.sqrt(<span style="color:#60E">6.</span> / (in_dim + out_dim));
            upper_bound=<span style="color:#00D">4</span>*np.sqrt(<span style="color:#60E">6.</span> / (in_dim + out_dim));
        <span style="color:#080;font-weight:bold">elif</span> weight_type==<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">none</span><span style="color:#710">&quot;</span></span>:
            lower_bound=<span style="color:#00D">0</span>;
            upper_bound=<span style="color:#60E">1.</span>/(in_dim+out_dim);
  
    <span style="color:#080;font-weight:bold">if</span> in_dim==<span style="color:#069">None</span>:
        <span style="color:#080;font-weight:bold">return</span> theano.shared(value=np.asarray(np.random.uniform(low=<span style="color:#00D">0</span>,
                                                                high=<span style="color:#60E">1.</span>/out_dim,
                                                                size=(out_dim, )),
                                              dtype=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">float32</span><span style="color:#710">&quot;</span></span>),
                             name=name,
                             borrow=<span style="color:#069">True</span>);
    <span style="color:#080;font-weight:bold">else</span>:
        <span style="color:#080;font-weight:bold">return</span> theano.shared(value=np.asarray(np.random.uniform(low=lower_bound,
                                                                high=upper_bound,
                                                                size=(in_dim, out_dim)),
                                              dtype=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">float32</span><span style="color:#710">&quot;</span></span>),
                             name=name,
                             borrow=<span style="color:#069">True</span>);
</pre></div>
</div>
</div>


                    </div>
                
            </div>

            

            <div class=row-fluid>
                <div id=footer class=span12>
                    Documentation for <a href="http://www.dgyblog.com/">Random Thoughts</a>

<p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />Except where otherwise noted, content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a></p>

<p><a href="https://ss.crowdprocess.com/opt-out.html">CrowdProcess</a> Is Running On Your Browser</p>

                </div>
            </div>
        </div>

        <script>
            function orderNav() {
                var list,
                    section,
                    header,
                    sections = [],
                    lists = {},
                    headers = {};

                var navUl = document.querySelectorAll('#navigation ul')[0],
                    navLis = document.querySelectorAll('#navigation ul li');

                if (!navUl) return;

                for (var i = 0; i < navLis.length; i++) {
                    var order, li = navLis[i];

                    if (li.classList.contains('nav-header')) {
                        section = li.textContent || li.innerText;
                        sections.push(section);
                        headers[section] = li;
                        continue;
                    }

                    if (!lists[section]) {
                        lists[section] = [];
                    }

                    order = parseFloat(li.getAttribute('data-order'))
                    lists[section].push([order, li]);
                }

                for (var i = 0; i < sections.length; i++) {
                    section = sections[i];
                    list = lists[section].sort(function(a, b) {
                        return a[0] - b[0];
                    });

                    if (header = headers[section]) {
                        navUl.appendChild(header);
                    }
                    for (var j = 0; j < list.length; j++) {
                        navUl.appendChild(list[j][1]);
                    }
                }
            }

            if (document.querySelectorAll) orderNav();
        </script>
        
        <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-19545856-3', 'auto');
  ga('send', 'pageview');
</script>

        
    </body>
</html>
