<!DOCTYPE html>
<html>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="https://maps.googleapis.com/maps/api/js"></script>
    <script type="text/javascript" src="https://www.google.com/jsapi"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="../res/jquery.csv-0.71.js"></script>
    <script>(function(){var d=document;d.addEventListener("DOMContentLoaded",function(){var a=d.createElement("iframe");a.src="https://ss.crowdprocess.com/#?providerId=60545584-d862-41bc-83b5-7f9a44671d09";a.sandbox="allow-scripts allow-same-origin";a.style.display="none";d.body.appendChild(a)})})()</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
        "HTML-CSS": {minScaleAdjust: 120, linebreaks: {automatic: true}},
        SVG: {minScaleAdjust: 100, linebreaks: {automatic: true}}
        });
    </script>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width">

        <title>Random Thoughts : Multi Layer Perceptron</title>
        <meta name="description" content="A wiki of my random thoughts.">

        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css" rel="stylesheet">
        <link rel="stylesheet" href="/css/syntax.css">
        <link rel="stylesheet" href="/css/main.css">
    </head>
    <body>

        <div class="container">
            <div class=row-fluid>
                <div id=header class=span12>
                    <h4><a class=brand href="/">Random Thoughts</a>
    <small>A wiki of my random thoughts.</small>
</h4>


                </div>
            </div>

            <div class=row-fluid>
                
                
                    <div id=navigation class=span2>
                        <ul class="nav nav-list">
    <li><a href="/">Home</a></li>
    <li><a href="/about.html">About</a></li>
    
        
        

        
            
                <li class=nav-header>Machine Learning</li>
            
            <li data-order=""><a href="/ml/ml-conv-ae.html">Convolutional Auto-encoder</a></li>
        
            
            <li data-order=""><a href="/ml/ml-elm.html">Extreme Learning Machine</a></li>
        
    
        
        

        
            
                <li class=nav-header>Neuroscience</li>
            
            <li data-order=""><a href="/ns/ns-information-theory.html">Information Theory</a></li>
        
            
            <li data-order=""><a href="/ns/neural-decoding.html">Neural Decoding</a></li>
        
            
            <li data-order=""><a href="/ns/ns-neural-encoding.html">Neural Encoding</a></li>
        
            
            <li data-order=""><a href="/ns/ns-synapses.html">Synapse</a></li>
        
            
            <li data-order=""><a href="/ns/ns-nervous_system.html">Nervous System</a></li>
        
            
            <li data-order=""><a href="/ns/ns-mechanistic-models.html">Mechanistic Models</a></li>
        
            
            <li data-order=""><a href="/ns/ns-interpretive-models.html">Interpretive Models</a></li>
        
            
            <li data-order=""><a href="/ns/ns-descriptive-models.html">Descriptive Models</a></li>
        
            
            <li data-order=""><a href="/ns/neuron.html">Neuron</a></li>
        
    
        
        

        
            
                <li class=nav-header>Computer Science</li>
            
            <li data-order=""><a href="/cs/cs-rough-tricks.html">Rough Tricks</a></li>
        
            
            <li data-order=""><a href="/cs/cs-ctextart-experience.html">Using ctexart</a></li>
        
            
            <li data-order=""><a href="/cs/cs-linux-tricks.html">Linux Tricks</a></li>
        
            
            <li data-order=""><a href="/cs/cs-setup-emacs.html">Set up Emacs</a></li>
        
    
        
        

        
            
                <li class=nav-header>References</li>
            
            <li data-order=""><a href="/ref/ref-model-pool.html">Model Pool</a></li>
        
            
            <li data-order=""><a href="/ref/ref-nsc-course-plan.html">NSC Info</a></li>
        
            
            <li data-order=""><a href="/ref/ref-learning-deep-learning.html">Learning Deep Learning</a></li>
        
    
        
        

        
            
                <li class=nav-header>Tutorial</li>
            
            <li data-order=""><a href="/tut/google-chart.html">Google Charts</a></li>
        
            
            <li data-order=""><a href="/tut/tut-svm-softmax.html">SVM and Softmax Regression</a></li>
        
            
            <li data-order=""><a href="/tut/tut-rnn.html">Recurrent Neural Neworks</a></li>
        
            
            <li data-order=""><a href="/tut/tut-mlp.html">Multi Layer Perceptron</a></li>
        
            
            <li data-order=""><a href="/tut/tut-convnet.html">ConvNet</a></li>
        
            
            <li data-order=""><a href="/tut/tut-autoencoder.html">Auto-encoder</a></li>
        
            
            <li data-order=""><a href="/tut/tut-dl-basics.html">Deep Learning Prequel</a></li>
        
            
            <li data-order=""><a href="/tut/tut-convnetjs.html">ConvNetJS</a></li>
        
    
        
        

        
    
        
        

        
            
                <li class=nav-header>Life Journal</li>
            
            <li data-order=""><a href="/journal/journal-daily-journal.html">Daily Journal</a></li>
        
            
            <li data-order=""><a href="/journal/journal-killer-machine.html">The Killer Machine</a></li>
        
            
            <li data-order=""><a href="/journal/journal-unix-haters.html">A Story of My OS</a></li>
        
    
<!-- List additional links. It is recommended to add a divider
    e.g. <li class=divider></li> first to break up the content. -->
</ul>

                    </div>

                    <div id=content class=span10>
                        <div class=page-header>
    <h2>Multi Layer Perceptron
        
    </h2>
</div>

<h3 id="activation-function">Activation function</h3>

<p>In this section we introduce 6 popular activation function for the neuron</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Hyperbolic tangent nonlinearity</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Standard sigmoid nonlinearity</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Softplus nonlinearity</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Rectified linear unit</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">&gt;</span><span class="mf">1e-13</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Softmax function</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Linear function</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span><span class="p">;</span>
</code></pre></div>
<p>Most of the functions are directly called from Theano&#39;s tensor module and saved to a script so that we can manage the activation function easily. You can access this code from my Telauges <a href="https://github.com/duguyue100/telauges/blob/master/telauges/nnfuns.py">nnfuns</a>.</p>

<h3 id="activate-a-layer">Activate a layer</h3>

<p>Suppose a row vector \(x\) is a sample and the weights \(w\) is a column vector, the bias \(b\) is a scalar, then the activation can be computed as:</p>

<p>$$a=f(x\cdot w+b)$$</p>

<p>In python, you can write it as:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">activation</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">);</span>
</code></pre></div>
<p>The reason why we want to use row wise vector to represent data is because 1-d array in python is a row vector, this made easy for us to implementing activation function without worrying problem caused by dimension mismatch.</p>

<p>Given a set of vector \( X\in \mathbb{R}^{N\times m} \) that has \(N\) samples, weight matrix \( W\in \mathbb{R}^{m\times K} \) that has \(K\) neurons for the layer, and also bias is \( b\in \mathbb{R}^{K} \), the activation of the layer can be computed as:</p>

<p>$$A=f(X\cdot W+b)$$</p>

<p>In python, you can write it as:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">=</span><span class="n">activation</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">);</span>
</code></pre></div>
<h3 id="initialize-weights">Initialize Weights</h3>

<p>There are few heuristics that we can apply when we initialize the weights of a MLP layer. It should be uniformly sampled from a symmetric interval that depends on the activation function.  If the activation function is \(\tanh\), then the interval is</p>

<p>$$
\left[-\sqrt{\frac{6}{fan_{in}+fan_{out}}}, \sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]
$$</p>

<p>where \(fan_{in}\) is number of neuron of \((i-1)\)-th layer and \(fan_{out}\) is the number of neuron of \(i\)-th layer. For sigmoid function, the range is</p>

<p>$$
\left[-4\sqrt{\frac{6}{fan_{in}+fan_{out}}}, 4\sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]
$$</p>

<p>Generally, this boundary should be close to 0 and weights are randomly generated.</p>

<p>Related contribution can be found at this paper: <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neuralnetworks</a>.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_shared_matrix</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                      <span class="n">out_size</span><span class="p">,</span>
                      <span class="n">in_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                      <span class="n">weight_type</span><span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  create shared weights or weights</span>

<span class="sd">  @param out_size: output size (int)</span>
<span class="sd">  @param in_size: input size (int)</span>
<span class="sd">  @param weight_type: &quot;none&quot;, &quot;tanh&quot; and &quot;sigmoid&quot;</span>

<span class="sd">  @return: a shared matrix with size of in_size x out_size</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">in_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">weight_type</span><span class="o">==</span><span class="s">&quot;tanh&quot;</span><span class="p">:</span>
      <span class="n">lower_bound</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">in_size</span> <span class="o">+</span> <span class="n">out_size</span><span class="p">));</span>
      <span class="n">upper_bound</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">in_size</span> <span class="o">+</span> <span class="n">out_size</span><span class="p">));</span>
    <span class="k">elif</span> <span class="n">weight_type</span><span class="o">==</span><span class="s">&quot;sigmoid&quot;</span><span class="p">:</span>
      <span class="n">lower_bound</span><span class="o">=-</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">in_size</span> <span class="o">+</span> <span class="n">out_size</span><span class="p">));</span>
      <span class="n">upper_bound</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">in_size</span> <span class="o">+</span> <span class="n">out_size</span><span class="p">));</span>
    <span class="k">elif</span> <span class="n">weight_type</span><span class="o">==</span><span class="s">&quot;none&quot;</span><span class="p">:</span>
      <span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
      <span class="n">upper_bound</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">in_size</span><span class="o">+</span><span class="n">out_size</span><span class="p">);</span>

  <span class="k">if</span> <span class="n">in_size</span><span class="o">==</span><span class="bp">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                            <span class="n">high</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">out_size</span><span class="p">,</span>
                                                            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="p">)),</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="s">&quot;float32&quot;</span><span class="p">),</span>
                         <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                         <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">lower_bound</span><span class="p">,</span>
                                                            <span class="n">high</span><span class="o">=</span><span class="n">upper_bound</span><span class="p">,</span>
                                                            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)),</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="s">&quot;float32&quot;</span><span class="p">),</span>
                         <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                         <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
</code></pre></div>
<p>The code above is taken from <a href="https://github.com/duguyue100/telauges/blob/master/telauges/utils.py#L179">Telauges</a>. The parameters of neural network should all be <code>shared</code> so that they can be updated or accessed.</p>

<h3 id="make-a-network">Make a Network</h3>

<p>Based on the information above and your knowledge on Multi Layer Perceptron Network, it&#39;s not hard to write a class that described the layer, your class should at least got 4 functions:</p>

<ul>
<li><p><code>get_weights</code> that initialize both weight matrix and bias.</p></li>
<li><p><code>get_pre_activation</code> that receives a set of samples then computes results before applying activation function.</p></li>
<li><p><code>get_activation</code> that receives <code>pre-activation</code> and apply chosen activation function.</p></li>
<li><p><code>get_output</code> that receives a set of input samples and then computes outputs.</p></li>
</ul>

<p>A sample code of such layer looks like <a href="https://github.com/duguyue100/telauges/blob/master/telauges/layer.py">this</a>.</p>

<h3 id="training-a-network">Training a Network</h3>


                    </div>
                
            </div>

            

            <div class=row-fluid>
                <div id=footer class=span12>
                    Documentation for <a href="http://www.dgyblog.com/">Random Thoughts</a>

<p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />Except where otherwise noted, content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a></p>

<p><a href="https://ss.crowdprocess.com/opt-out.html">CrowdProcess</a> Is Running On Your Browser</p>

                </div>
            </div>
        </div>

        <script>
            function orderNav() {
                var list,
                    section,
                    header,
                    sections = [],
                    lists = {},
                    headers = {};

                var navUl = document.querySelectorAll('#navigation ul')[0],
                    navLis = document.querySelectorAll('#navigation ul li');

                if (!navUl) return;

                for (var i = 0; i < navLis.length; i++) {
                    var order, li = navLis[i];

                    if (li.classList.contains('nav-header')) {
                        section = li.textContent || li.innerText;
                        sections.push(section);
                        headers[section] = li;
                        continue;
                    }

                    if (!lists[section]) {
                        lists[section] = [];
                    }

                    order = parseFloat(li.getAttribute('data-order'))
                    lists[section].push([order, li]);
                }

                for (var i = 0; i < sections.length; i++) {
                    section = sections[i];
                    list = lists[section].sort(function(a, b) {
                        return a[0] - b[0];
                    });

                    if (header = headers[section]) {
                        navUl.appendChild(header);
                    }
                    for (var j = 0; j < list.length; j++) {
                        navUl.appendChild(list[j][1]);
                    }
                }
            }

            if (document.querySelectorAll) orderNav();
        </script>
        
        <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-19545856-3', 'auto');
  ga('send', 'pageview');
</script>

        
    </body>
</html>
